{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dedbe889",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\indra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\indra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\indra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d45555c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\indra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('omw-1.4')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_data(new_df):\n",
    "    remove_html_tags='<.*?>';\n",
    "    remove_urls='http\\S+';\n",
    "    remove_non_alpha='[^A-Za-z ]'\n",
    "    remove_extra_space=' +'\n",
    "    processed=[]\n",
    "    new_df=list(new_df)\n",
    "    for i in range(len(new_df)):\n",
    "        s=str(new_df[i])\n",
    "        if s==\"\":\n",
    "            continue\n",
    "        s=re.sub(remove_html_tags,\"\",s)\n",
    "        s=re.sub(remove_urls,\"\",s)\n",
    "        s=re.sub(remove_non_alpha,\"\",s)\n",
    "        s=re.sub(remove_extra_space,\" \",s)\n",
    "        if s==\"\":\n",
    "            continue\n",
    "        processed.append(s)\n",
    "        \n",
    "        \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    final_processed_text=[]\n",
    "    \n",
    "    for i in range(len(processed)):\n",
    "        s=processed[i]\n",
    "        s=s.split(\" \")\n",
    "        s=[ps.stem(word) for word in s if word not in stop_words]\n",
    "        final_processed_text.append(s)\n",
    "        \n",
    "    return final_processed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d8885ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.wordmap={}\n",
    "        self.wordcount=0\n",
    "        self.occurences={}\n",
    "        self.total_sentences=0\n",
    "    \n",
    "    def create(self,data):\n",
    "        \n",
    "        for sentence in data:\n",
    "            \n",
    "            for i in sentence+[\"end\"]:\n",
    "                i=i.lower()\n",
    "                \n",
    "                if i not in self.wordmap:\n",
    "                    self.wordmap[i]=[self.wordcount,0]\n",
    "                    self.wordcount+=1\n",
    "                self.wordmap[i][1]+=1\n",
    "    \n",
    "    def get_term_frequency(self,sentence,word):\n",
    "        \n",
    "        sentence=[i.lower() for i in sentence]\n",
    "        return sentence.count(word)/len(sentence)\n",
    "        \n",
    "                    \n",
    "                \n",
    "    def get_inverse_document_frequency(self,word):\n",
    "        inv_doc_freq=np.log(self.total_sentences/self.wordmap[word][1])\n",
    "        return inv_doc_freq\n",
    "                \n",
    "    def get_embeddings(self,sentence):\n",
    "        \n",
    "\n",
    "        temp=[0]*self.wordcount\n",
    "        for i in sentence:\n",
    "            i=i.lower()\n",
    "            if i in self.wordmap:\n",
    "                temp[self.wordmap[i][0]]=self.get_term_frequency(sentence,i)*self.get_inverse_document_frequency(i)\n",
    "        \n",
    "        return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfffb22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id label1 label2                                               text\n",
      "0    07Zfn0z   Fake    Pos  [if, your, look, eleg, hotel, downtown, chicag...\n",
      "1    08HSeiI   Fake    Pos  [the, atmospher, talbott, hotel, welcom, first...\n",
      "2    0L52Itl   True    Neg  [id, search, cool, nonchain, hotel, weekend, g...\n",
      "3    0LcSUgS   True    Pos  [i, vacat, fairmont, chicago, night, juli, the...\n",
      "4    0N9L6lV   Fake    Neg  [the, fairmont, chicago, millennium, park, one...\n",
      "..       ...    ...    ...                                                ...\n",
      "955  zTPdVsT   Fake    Pos  [the, talbot, hotel, eleg, place, take, wife, ...\n",
      "956  zWuJa6N   Fake    Neg  [my, husband, i, recent, stay, fairmont, chica...\n",
      "957  zfeuazq   True    Neg  [i, surpris, fact, extra, sheet, blanket, make...\n",
      "958  zj2gpGP   True    Neg  [i, reserv, rock, star, suit, boyfriend, birth...\n",
      "959  zwf3FEc   True    Neg  [i, expect, glamor, room, i, walk, disappoint,...\n",
      "\n",
      "[960 rows x 4 columns]           id                                               text label1 label2\n",
      "0    0EL4s2q  [i, recent, stay, sheraton, chicago, hotel, it...   Fake    Neg\n",
      "1    0UEraLY  [i, book, via, pricelin, sure, id, like, shera...   True    Pos\n",
      "2    0cKBWf6  [after, stay, sheraton, royal, orchid, hotel, ...   True    Neg\n",
      "3    0gQlq5g  [we, absolut, love, knickerbock, now, expect, ...   True    Pos\n",
      "4    0lsa78x  [i, want, everyon, know, aw, experi, i, sherat...   Fake    Neg\n",
      "..       ...                                                ...    ...    ...\n",
      "315  z3UfsQ0  [thi, hotel, incred, havent, better, stay, hot...   Fake    Pos\n",
      "316  zDZ9mKe  [we, stay, hotel, confer, the, lobbi, confer, ...   True    Neg\n",
      "317  zHsSZo9  [my, famili, realli, enjoy, hotel, weekend, ch...   Fake    Pos\n",
      "318  zSLt5zp  [i, travel, illinoi, uk, busi, book, friday, s...   True    Pos\n",
      "319  zhCIGzg  [my, famili, i, stay, visit, chicago, thi, per...   Fake    Pos\n",
      "\n",
      "[320 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "f=open(\"./data/train-labeled.txt\",'r')\n",
    "test_f=open(\"./data/dev-text.txt\")\n",
    "key_f=open(\"./data/dev-key.txt\")\n",
    "\n",
    "hashmap={\"id\":[],\"label1\":[],\"label2\":[],\"text\":[]}\n",
    "for i in f.readlines():\n",
    "    data=i.split(\" \")\n",
    "    hashmap[\"id\"].append(data[0])\n",
    "    hashmap[\"label1\"].append(data[1])\n",
    "    hashmap[\"label2\"].append(data[2])\n",
    "    hashmap[\"text\"].append(\" \".join(data[3:]).rstrip())\n",
    "df=pd.DataFrame(hashmap)\n",
    "df['text']=preprocess_data(df['text'])\n",
    "\n",
    "key={i.split(\" \")[0]:i.split(\" \")[1:] for j in range(2) for i in key_f.readlines()}\n",
    "test_map={\"id\":[],\"text\":[],\"label1\":[],\"label2\":[]}\n",
    "for i in test_f.readlines():\n",
    "    data=i.split(\" \")\n",
    "    test_map[\"id\"].append(data[0])\n",
    "    test_map[\"text\"].append(\" \".join(data[1:]))\n",
    "    test_map[\"label1\"].append(key[data[0]][0])\n",
    "    test_map[\"label2\"].append(key[data[0]][1].rstrip())\n",
    "    \n",
    "test_df=pd.DataFrame(test_map)\n",
    "test_df['text']=preprocess_data(test_df['text'])\n",
    "print(df,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56cab30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 413\n"
     ]
    }
   ],
   "source": [
    "minlen=float(\"inf\")\n",
    "maxlen=float(\"-inf\")\n",
    "for i in df['text']:\n",
    "    minlen=min(minlen,len(i))\n",
    "    maxlen=max(maxlen,len(i))\n",
    "print(minlen,maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c56796fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader=DataLoader()\n",
    "dataloader.create(df['text'])\n",
    "dataloader.total_sentences=len(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d2d0e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_embeddings(Xtrain,Xtest,fixedsize=0,dataloader=None):\n",
    "    Xtrain_embeddings=[]\n",
    "    Xtest_embeddings=[]\n",
    "    \n",
    "\n",
    "    for i in Xtrain:\n",
    "        Xtrain_embeddings.append(dataloader.get_embeddings(i))\n",
    "        \n",
    "    for i in Xtest:\n",
    "            \n",
    "        Xtest_embeddings.append(dataloader.get_embeddings(i))\n",
    "    \n",
    "    \n",
    "    return Xtrain_embeddings,Xtest_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "955beae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df=df.sample(frac=1)\n",
    "# train_df=df\n",
    "Xtrain,ytrain1,ytrain2=train_df['text'],train_df['label1'],train_df['label2']\n",
    "Xtest,ytest1,ytest2=test_df['text'],test_df['label1'],test_df['label2']\n",
    "Xtrain_emb,Xtest_emb=get_word2vec_embeddings(Xtrain,Xtest,fixedsize=200,dataloader=dataloader)\n",
    "Xtrain_emb=np.asarray(Xtrain_emb)\n",
    "Xtest_emb=np.asarray(Xtest_emb)\n",
    "classmap={\"Fake\":-1,\"Neg\":-1,\"Pos\":1,\"True\":1}\n",
    "ytrain1=[classmap[i] for i in ytrain1]\n",
    "ytrain2=[classmap[i] for i in ytrain2]\n",
    "ytest1=[classmap[i] for i in ytest1]\n",
    "ytest2=[classmap[i] for i in ytest2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Node:\n",
    "    \n",
    "    def __init__(self,w_size=1000,bias=0):\n",
    "        \n",
    "        self.w=np.asarray([0]*w_size)\n",
    "        self.bias=bias\n",
    "    \n",
    "    def __init__(self,w_size=1000,bias=0,avg=True):\n",
    "        self.w=np.asarray([0]*w_size)\n",
    "        self.bias=bias\n",
    "        self.cw=np.asarray([0]*w_size)\n",
    "        self.cbias=bias\n",
    "        \n",
    "class Perceptron:\n",
    "    def __init__(self,model_type='vanilla',emb_size=4500,bias=0,n=2):\n",
    "        self.emb_size=emb_size\n",
    "        self.model_type=model_type\n",
    "        self.perceptrons=[Node(emb_size,bias),Node(emb_size,bias,True)]\n",
    "        \n",
    "        \n",
    "    def forward(self,inp):\n",
    "        out=[]\n",
    "        for i in range(len(self.perceptrons)):\n",
    "            prod=np.dot(inp,self.perceptrons[i].w)\n",
    "            s=prod+self.perceptrons[i].bias\n",
    "            out.append(s)\n",
    "        return out\n",
    "    \n",
    "    def train(self,data,label1,label2):\n",
    "\n",
    "        if self.model_type=='vanilla':\n",
    "\n",
    "            for inp,lab1,lab2 in zip(data,label1,label2):\n",
    "                out=self.forward(np.asarray(inp))\n",
    "                lab=[lab1,lab2]\n",
    "                for i in range(len(out)):\n",
    "                    if out[i]*lab[i]<=0:\n",
    "                        self.perceptrons[i].w=self.perceptrons[i].w+inp*lab[i]\n",
    "                        self.perceptrons[i].bias=self.perceptrons[i].bias+lab[i]\n",
    "        else:\n",
    "            counter=0\n",
    "            for inp,lab1,lab2 in zip(data,label1,label2):\n",
    "                out=self.forward(np.asarray(inp))\n",
    "                lab=[lab1,lab2]\n",
    "                if out[0]*lab[0]<=0:\n",
    "                    \n",
    "                    self.perceptrons[0].w=self.perceptrons[0].w+inp*lab[0]\n",
    "                    \n",
    "                    self.perceptrons[0].bias=self.perceptrons[0].bias+lab[0]\n",
    "                    \n",
    "#                     self.perceptrons[0].cw=self.perceptrons[0].cw+lab[0]*counter*inp\n",
    "                    \n",
    "#                     self.perceptrons[0].cbias=self.perceptrons[0].cbias+lab[0]*counter\n",
    "                    self.perceptrons[0].cw=self.perceptrons[0].cw+self.perceptrons[0].w\n",
    "                    \n",
    "                    self.perceptrons[0].cbias=self.perceptrons[0].cbias+self.perceptrons[0].bias\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                if out[1]*lab[1]<=0:\n",
    "                    \n",
    "                    self.perceptrons[1].w=self.perceptrons[1].w+inp*lab[1]\n",
    "                    \n",
    "                    self.perceptrons[1].bias=self.perceptrons[1].bias+lab[1]\n",
    "                    \n",
    "#                     self.perceptrons[1].cw=self.perceptrons[1].cw+lab[1]*counter*inp\n",
    "                    \n",
    "#                     self.perceptrons[1].cbias=self.perceptrons[1].cbias+lab[1]*counter\n",
    "                    \n",
    "                    self.perceptrons[1].cw=self.perceptrons[1].cw+self.perceptrons[1].w\n",
    "                    \n",
    "                    self.perceptrons[1].cbias=self.perceptrons[1].cbias+self.perceptrons[1].bias\n",
    "                    \n",
    "                counter+=1\n",
    "\n",
    "            self.perceptrons[0].w=(1/counter)*self.perceptrons[0].cw\n",
    "            \n",
    "            self.perceptrons[0].bias=(1/counter)*self.perceptrons[0].cbias\n",
    "            \n",
    "            self.perceptrons[1].w=(1/counter)*self.perceptrons[1].cw\n",
    "            \n",
    "            self.perceptrons[1].bias=(1/counter)*self.perceptrons[1].cbias\n",
    "\n",
    "             \n",
    "                        \n",
    "    def predict(self,data):\n",
    "        out=[]\n",
    "        for i in data:\n",
    "            out.append(self.forward(i))\n",
    "        pred1=[]\n",
    "        pred2=[]\n",
    "        for i in out:\n",
    "            temp=[1,1]\n",
    "            if i[0]<0:\n",
    "                temp[0]=-1\n",
    "            if i[1]<0:\n",
    "                temp[1]=-1\n",
    "            pred1.append(temp[0])\n",
    "            pred2.append(temp[1])\n",
    "        return pred1,pred2\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e92c6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5795"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7c9791c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6504065040650407 0.9240121580547113\n"
     ]
    }
   ],
   "source": [
    "p=Perceptron(emb_size=dataloader.wordcount)\n",
    "\n",
    "epoch_count=500\n",
    "\n",
    "for i in range(epoch_count):\n",
    "    \n",
    "    p.train(Xtrain_emb,ytrain1,ytrain2)\n",
    "    \n",
    "\n",
    "pred=p.predict(Xtest_emb)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(f1_score(ytest1,pred[0]),f1_score(ytest2,pred[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "231b2a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8023598820058997 0.9184290030211479\n"
     ]
    }
   ],
   "source": [
    "p=Perceptron(emb_size=dataloader.wordcount,model_type='average')\n",
    "\n",
    "epoch_count=500\n",
    "\n",
    "for i in range(epoch_count):\n",
    "\n",
    "    p.train(Xtrain_emb,ytrain1,ytrain2)\n",
    "\n",
    "pred=p.predict(Xtest_emb)\n",
    "print(f1_score(ytest1,pred[0]),f1_score(ytest2,pred[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e243b3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Node at 0x2d30f714c48>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.perceptrons[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c0c160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
