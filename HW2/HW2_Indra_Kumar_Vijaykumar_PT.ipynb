{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e1cecc6",
   "metadata": {},
   "source": [
    "#### Import Libraries\n",
    "\n",
    "All the libraries used are of the latest version. So if any library doesn't exist, the version that comes with a simple \"pip install\" should suffice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64f44dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\indra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\indra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\indra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "G:\\Pytorch_practice\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "nltk.download('punkt')\n",
    "import gensim.downloader\n",
    "#!pip install gensim\n",
    "import gensim\n",
    "import gensim.downloader\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a82e2f8",
   "metadata": {},
   "source": [
    "#### Import the data and select the review_body and ratings columns\n",
    "\n",
    "Data is imported from the \"data.tsv\" file present in the same folder as this ipynb file. Pandas is the library used to read the data file. The \"on_bad_lines\" parameter skips the lines that have issues and throws and error while reading the data file such as missing values. The columns \"review_body\" and \"star_rating\" are taken from the data and used in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc0fe06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Pytorch_practice\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3457: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>star_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1550488</th>\n",
       "      <td>I love the ring, it does get caught on things ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1740381</th>\n",
       "      <td>The chain broke first day she wore it. We cant...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573293</th>\n",
       "      <td>This is pathetic and illegal to take advantage...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1642751</th>\n",
       "      <td>Poor choice, poor quality of the pendant and t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213501</th>\n",
       "      <td>What a waste of money, should of went to the m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review_body star_rating\n",
       "1550488  I love the ring, it does get caught on things ...         1.0\n",
       "1740381  The chain broke first day she wore it. We cant...           1\n",
       "1573293  This is pathetic and illegal to take advantage...         1.0\n",
       "1642751  Poor choice, poor quality of the pendant and t...         1.0\n",
       "1213501  What a waste of money, should of went to the m...           1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org_df=pd.read_csv(\"./data.tsv\",sep='\\t',on_bad_lines=\"skip\")\n",
    "required_columns=['review_body','star_rating']\n",
    "df=org_df[required_columns]\n",
    "new_df=pd.DataFrame({\"review_body\":[],\"star_rating\":[]})\n",
    "for i in range(1,6):\n",
    "    new_df=pd.concat([new_df,df.loc[df['star_rating']==i].sample(50000)])\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c739b94",
   "metadata": {},
   "source": [
    "#### Function to preprocess the data\n",
    "\n",
    "The data is preprocessed here to improve the quality of results. The steps followed in preprocessing the data in the \"review_body\" column are as follows:-\n",
    "\n",
    "-> Removing html tags, urls, non-alphabetic characters other than space, extra space characters, are removed from the data using the regex.\n",
    "\n",
    "-> Stopwords are removed from the data using the stopwords dictionary from the nltk library.\n",
    "\n",
    "-> The inword2vec parameter is used as a flag which if set to True performs filtration in the data and retains only those words in the sentence that have corresponding word embeddings in the word2vec model.\n",
    "\n",
    "-> The wordvec parameter takes as input the word2vec model which is used to check if a word is present as a key value in the model. This check is done only if the inword2vec parameter is set to the value True.\n",
    "\n",
    "->The fixed size parameter is used to filter the sentences based on their length. Only those sentences are allowed to become a part of the final processed data whose number of words are >= fixedsize value.\n",
    "\n",
    "-> The df parameter takes as input the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e1ebd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing\n",
    "\n",
    "def preprocess_data(df,inword2vec=False,wordvec=None,fixedsize=None):\n",
    "    remove_html_tags='<.*?>';\n",
    "    remove_urls='http\\S+';\n",
    "    remove_non_alpha='[^A-Za-z ]'\n",
    "    remove_extra_space=' +'\n",
    "    processed={\"review_body\":[],\"star_rating\":[]}\n",
    "    for i in range(len(new_df)):\n",
    "        s=str(new_df['review_body'].iloc[i])\n",
    "        c=new_df['star_rating'].iloc[i]\n",
    "        if s==\"\":\n",
    "            continue\n",
    "        s=re.sub(remove_html_tags,\"\",s)\n",
    "        s=re.sub(remove_urls,\"\",s)\n",
    "        s=re.sub(remove_non_alpha,\"\",s)\n",
    "        s=re.sub(remove_extra_space,\" \",s)\n",
    "        if s==\"\":\n",
    "            continue\n",
    "        processed[\"review_body\"].append(contractions.fix(s.lower()))\n",
    "        processed[\"star_rating\"].append(int(c))\n",
    "        \n",
    "        \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    final_processed_text={\"input\":[],\"target\":[]}\n",
    "    count={1:0,2:0,3:0,4:0,5:0}\n",
    "    \n",
    "    for i in range(len(processed['review_body'])):\n",
    "        s=processed['review_body'][i]\n",
    "        c=processed['star_rating'][i]\n",
    "        if count[int(c)]>=20000:\n",
    "            continue\n",
    "        s=s.split(\" \")\n",
    "        s=[word for word in s if word not in stop_words]\n",
    "        if inword2vec:\n",
    "            s=[word for word in s if word in wordvec.key_to_index]\n",
    "        if not fixedsize:\n",
    "            if len(s)<13:\n",
    "                continue\n",
    "            s=\" \".join(s)\n",
    "            tokenize_words=word_tokenize(s)\n",
    "            final_processed_text['input'].append(tokenize_words)\n",
    "            final_processed_text['target'].append(c)\n",
    "            count[int(c)]+=1\n",
    "        else:\n",
    "            if len(s)>=fixedsize:\n",
    "                s=\" \".join(s)\n",
    "                tokenize_words=word_tokenize(s)\n",
    "                final_processed_text['input'].append(tokenize_words)\n",
    "                final_processed_text['target'].append(c)\n",
    "                count[int(c)]+=1\n",
    "                \n",
    "    return final_processed_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf2e2df",
   "metadata": {},
   "source": [
    "#### Function to generate word2vec based word embeddings for the input data\n",
    "\n",
    "The following function generates word2vec based word embeddings for the input data. The outputs are of three types:-\n",
    "\n",
    "-> Each training sample is a mean vector embedding of the sentence in the sample. This happens when skipten==False.\n",
    "\n",
    "-> Each training sample consists of a sublist of wordvectors and the number of words taken from the sentence is fixed. This happens when skipten==True. The fixedsize contains the value which is the required number of words from the sentence. Here the parameter \"concat is set to False\" as each word vector in the sample is present inside its own list and not concatenated with the rest of the word vectors. Thus each train sample with be list of lists.\n",
    "\n",
    "-> Each training sample consists of a list that has all the word vectors concatenated with each other and the number of words taken from the sentence is fixed. This happens when skipten==True. The fixedsize contains the value which is the required number of words from the sentence. Here the parameter \"concat is set to True\" as each word vector is concatenated with the rest of the word vectors.\n",
    "\n",
    "-> The parameter long when set to true makes the 0's and 1's in the one hot encoding of the labels to be of datatype long and when it is set to false, the datatype of the 0's and 1's is float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fb62bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The role of the label encoding function is to generate one hot encoded labels i.e for ex : 5 -> [0,0,0,0,1]   \n",
    "def label_encoding(arr,skipped,long=False):\n",
    "    res=[]\n",
    "    skipped=set(skipped)\n",
    "    count=0\n",
    "    for i in arr:\n",
    "        if count in skipped:\n",
    "            count+=1\n",
    "            continue\n",
    "        temp=[0]*5\n",
    "        if long:\n",
    "            temp[i-1]=1\n",
    "        else:\n",
    "            temp[i-1]=float(1)\n",
    "        res.append(temp)\n",
    "        count+=1\n",
    "    return res\n",
    "    \n",
    "\n",
    "def get_word2vec_embeddings(Xtrain,Xtest,ytrain,ytest,skipten=False,fixedsize=None,long=False,concat=False,w2model_pretrained=None):\n",
    "    Xtrain_embeddings=[]\n",
    "    Xtest_embeddings=[]\n",
    "    \n",
    "    skipped_train_sample=[] ### This list contain those training samples in which none of the words are present as a key in the- \n",
    "    ##- word2vec model and therefore need to be skipped as they become noise in the final data with only 0 as vector values.\n",
    "    \n",
    "    skipped_test_sample=[]  ### This list contain those testing samples in which none of the words are present as a key in the- \n",
    "    ##- word2vec model and therefore need to be skipped as they become noise in the final data with only 0 as vector values.\n",
    "    \n",
    "    \n",
    "    if not skipten: ## This if condition controls whether for each sentence we calculate the mean vector of the word embeddings -\n",
    "        ## - or we add the vector of each word as a part of the training/testing sample. If skipten is False, we add the meanvect-\n",
    "        ## -or of a sentence.\n",
    "        \n",
    "        for i in Xtrain:\n",
    "            temp=[] ### A temporary array to accumulate the vectors of all the words in a sentence or one training sample.\n",
    "            for j in i:\n",
    "                if j in w2model_pretrained.key_to_index:\n",
    "                    temp.append(w2model_pretrained.get_vector(j))\n",
    "        \n",
    "            Xtrain_embeddings.append(w2model_pretrained.get_mean_vector(temp)) ### Mean vector of the accumulated word vectors in -\n",
    "            ## - the temp array is added to the final train embeddings array.\n",
    "            \n",
    "        ## The same steps as above but for the test data\n",
    "        for i in Xtest:\n",
    "            temp=[]\n",
    "            for j in i:\n",
    "                if j in w2model_pretrained.key_to_index:\n",
    "                    temp.append(w2model_pretrained.get_vector(j))\n",
    "            Xtest_embeddings.append(w2model_pretrained.get_mean_vector(temp))\n",
    "            \n",
    "    else: ### The following code are executed if each word vector has to be a part of the training data and not the mean vector.\n",
    "        \n",
    "        ### The concat parameter controls if the word vectors in a train sample are concatenated with each other to form one -\n",
    "        ###- array when it is set as true or each word vector is present as a separate array making one train sample an array with-\n",
    "        ### subarrays containing each word vector.\n",
    "        \n",
    "        if not concat: ## When concat is set to False \n",
    "    \n",
    "            for i in range(len(Xtrain)):\n",
    "            \n",
    "                steps=fixedsize ### fixedsize parameter controls the no of word we want from the train sample\n",
    "                temp=[] ### A list containing sublists where each sublist contains the corresponding word vector\n",
    "            \n",
    "                for j in Xtrain[i]:\n",
    "                    if steps==0: ## when the required number of words is reached the loop breaks\n",
    "                        break\n",
    "                        \n",
    "                    if j in w2model_pretrained.key_to_index:\n",
    "                        temp.append(w2model_pretrained.get_vector(j))\n",
    "                        steps-=1\n",
    "                        \n",
    "                if len(temp)==0: ## if none of the words are present as a key in the word2vec model, the temp list is empty so \n",
    "                    ## the train sample is skipped to avoid noise and is added to the skipped_train_sample list so that the corre-\n",
    "                    ## -sponding labels will be skipped as well\n",
    "                    \n",
    "                    skipped_train_sample.append(i)\n",
    "                    continue\n",
    "                    \n",
    "                    \n",
    "                if len(temp)<fixedsize: ## if the length of the temp list is not 0 but only a few word vectors short of the \n",
    "                    ## fixed size the rest of the values are added as 0 vectors of size 300 to match the embedding lengths of\n",
    "                    ## rest of the sublists.\n",
    "                    \n",
    "                    temp=temp+[[0.0]*300]*(fixedsize-len(temp))\n",
    "                Xtrain_embeddings.append(temp)\n",
    "            \n",
    "            ## The same steps as above are followed but for the testing data\n",
    "            for i in range(len(Xtest)):\n",
    "                steps=fixedsize\n",
    "                temp=[]\n",
    "                for j in Xtest[i]:\n",
    "                    if steps==0:\n",
    "                        break\n",
    "                    if j in w2model_pretrained.key_to_index:\n",
    "                        temp.append(w2model_pretrained.get_vector(j))\n",
    "                        steps-=1\n",
    "                if len(temp)==0:\n",
    "                    skipped_train_sample.append(i)\n",
    "                    continue\n",
    "                if len(temp)<fixedsize:\n",
    "                    temp=temp+[[0.0]*300]*(fixedsize-len(temp))\n",
    "\n",
    "                Xtest_embeddings.append(temp)\n",
    "        \n",
    "        else: ## The following code executes when concat is True and therefore the word vectors have to be concatenated with\n",
    "            ## each other and therefore each training sample is one list having all the word vectors contenated together as input.\n",
    "            \n",
    "            for i in range(len(Xtrain)):\n",
    "                steps=fixedsize\n",
    "                temp=np.array([],dtype=np.float32) ## An empty numpy array to which all the word vectors are concatenated \n",
    "                for j in Xtrain[i]:\n",
    "                    if steps==0:## when the required number of words is reached the loop breaks\n",
    "                        break\n",
    "                    if j in w2model_pretrained.key_to_index:\n",
    "                        temp=np.concatenate([temp,w2model_pretrained.get_vector(j)]) ## if the word is present as a key in the wo-\n",
    "                        ##rd2vec model, it is concatenated to the temp array\n",
    "                        steps-=1\n",
    "                        \n",
    "                Xtrain_embeddings.append(temp)\n",
    "            \n",
    "            ## The same steps as above are followed but for the testing data\n",
    "            for i in range(len(Xtest)):\n",
    "                steps=fixedsize\n",
    "                temp=np.array([],dtype=np.float32)\n",
    "                for j in Xtest[i]:\n",
    "                    if steps==0:\n",
    "                        break\n",
    "                    if j in w2model_pretrained.key_to_index:\n",
    "                        temp=np.concatenate([temp,w2model_pretrained.get_vector(j)])\n",
    "                        steps-=1\n",
    "                Xtest_embeddings.append(temp)\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "   \n",
    "    ytrain_en=label_encoding(ytrain,skipped_train_sample,long)\n",
    "    ytest_en=label_encoding(ytest,skipped_test_sample,long)\n",
    "    \n",
    "    return Xtrain_embeddings,Xtest_embeddings,ytrain_en,ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2098203b",
   "metadata": {},
   "source": [
    "#### A function that returns the corresponding tf-idf word embeddings for given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "112b3b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_embeddings(data):\n",
    "    tfidf = TfidfVectorizer()\n",
    "    vector_rep=tfidf.fit_transform(data)\n",
    "    return vector_rep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7b1612",
   "metadata": {},
   "source": [
    "### 1)\n",
    "\n",
    "Fetching preprocessed data and splitting it into training and testing set data with a split of 80% and 20% respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d50ace70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_data=preprocess_data(new_df)\n",
    "Xtrain,Xtest,ytrain,ytest=train_test_split(processed_data['input'],processed_data['target'],test_size=0.2,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0913951",
   "metadata": {},
   "source": [
    "### 2a)\n",
    "\n",
    "Here the pretrained word2vec-google-news-300 model is used to checkout semantic similarities for a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3869ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A pretrained word2vec model used to generate the word embeddings required to train the Machine learning and Deep learning models\n",
    "\n",
    "w2model_pretrained=gensim.downloader.load('word2vec-google-news-300')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c104bfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A function that returns the cosine similarity between two vectors\n",
    "def get_cosine_similarity(a,b):\n",
    "    return np.dot(a,b)/((np.linalg.norm(a))*np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dd2a70a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44240144"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Examples\n",
    "## Cosine similarity between (King-Man+Woman) and (Queen)\n",
    "a=w2model_pretrained.get_vector('King')-w2model_pretrained.get_vector('Man')+w2model_pretrained.get_vector('Woman')\n",
    "b=w2model_pretrained.get_vector('Queen')\n",
    "get_cosine_similarity(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afedb72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28467536"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Cosine similarity between the words Cute and Beautiful\n",
    "\n",
    "a=w2model_pretrained.get_vector('Cute')\n",
    "b=w2model_pretrained.get_vector('Beautiful')\n",
    "get_cosine_similarity(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c04f74a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5888708"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Cosine similarity between the vectors from (Daughter-Woman+man) and (Son)\n",
    "\n",
    "a=w2model_pretrained.get_vector('Daughter')-w2model_pretrained.get_vector('Woman')+w2model_pretrained.get_vector('Man')\n",
    "b=w2model_pretrained.get_vector('Son')\n",
    "get_cosine_similarity(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec7cb93",
   "metadata": {},
   "source": [
    "### 2b)\n",
    "Here a word2vec model is trained on the Amazon review dataset to checkout the semantic similarities learned by this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ac151d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56895372, 65218680)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Own model\n",
    "w2model = Word2Vec(min_count=5,window=11,vector_size=300,workers=1)\n",
    "w2model.build_vocab(processed_data['input'])\n",
    "w2model.train(Xtrain,total_examples=w2model.corpus_count,epochs=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20b983b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18310976"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Cosine similarity between (King-Man+Woman) and (Queen)\n",
    "\n",
    "a=w2model.wv.get_vector('king')-w2model.wv.get_vector('man')+w2model.wv.get_vector('woman')\n",
    "b=w2model.wv.get_vector('queen')\n",
    "get_cosine_similarity(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4479676e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30222067"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Cosine similarity between the words Cute and Beautiful\n",
    "\n",
    "a=w2model.wv.get_vector('cute')\n",
    "b=w2model.wv.get_vector('beautiful')\n",
    "get_cosine_similarity(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a13c4dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45625308"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Cosine similarity between the vectors from (Daughter-Woman+man) and (Son)\n",
    "\n",
    "a=w2model.wv.get_vector('daughter')-w2model.wv.get_vector('woman')+w2model.wv.get_vector('man')\n",
    "b=w2model.wv.get_vector('son')\n",
    "get_cosine_similarity(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08164f",
   "metadata": {},
   "source": [
    "It is clear from the above comparisons in the semantic similarities that the pretrained word2vec-google-news-300 model performs better than the newly trained model which could be because of the smaller amount of data used to train the latter model. But the similarity values for words that could possibly have a higher frequency in the Amazon reviews training data yields results very close to that of the pretrained word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40a0236",
   "metadata": {},
   "source": [
    "### 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd901e2",
   "metadata": {},
   "source": [
    "Perceptron and SVM prediction results using the word2vec model trained on the given Amazon reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1b73ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the word vector embeddings using the word2vec model we trained on the Amazon reviews dataset. Ytest value is not taken as\n",
    "## vector embeddings below because it can simply be used in its original integer value format.\n",
    "Xtrain_embeddings,Xtest_embeddings,ytrain_en,_=get_word2vec_embeddings(Xtrain,Xtest,ytrain,ytest,w2model_pretrained=w2model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b21539b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40805"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perceptron results\n",
    "perceptron=Perceptron()\n",
    "perceptron.fit(Xtrain_embeddings,ytrain)\n",
    "perceptron.score(Xtest_embeddings,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da6e281d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Pytorch_practice\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5135"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM results\n",
    "svm=LinearSVC()\n",
    "svm.fit(Xtrain_embeddings,ytrain)\n",
    "svm.score(Xtest_embeddings,ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c67adf8",
   "metadata": {},
   "source": [
    "Perceptron and SVM prediction results using the tf-idf embeddings from the given Amazon reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aeb14e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the word vector embeddings using the tf-idf model based on the Amazon reviews dataset\n",
    "## Here the train_test_split is done again, as the tf-idf model is fit on the entire processed_data because if it is only fit \n",
    "## on the train data, then there can be words in test data that are not present in train data, which can cause an error \n",
    "## when transforming the test data.\n",
    "\n",
    "processed_data_as_sentence=[\" \".join(i) for i in processed_data['input']]\n",
    "processed_data_tfidf_vectors=tf_idf_embeddings(processed_data_as_sentence)\n",
    "Xtrain_embeddings,Xtest_embeddings,ytrain,ytest=train_test_split(processed_data_tfidf_vectors,processed_data['target'],shuffle=True,test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2189faf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42195"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Perceptron results\n",
    "perceptron=Perceptron()\n",
    "perceptron.fit(Xtrain_embeddings,ytrain)\n",
    "perceptron.score(Xtest_embeddings,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73edfe03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5002"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVM results\n",
    "svm=LinearSVC()\n",
    "svm.fit(Xtrain_embeddings,ytrain)\n",
    "svm.score(Xtest_embeddings,ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af71791",
   "metadata": {},
   "source": [
    "Its clear from the above results from the comparison between tfidf and word2vec, that word2vec model performs better than the tfidf model. Though the results of the perceptron are very close to each other and the tfidf exceeds in performance only by a very small amount, that can be attributed to reasons like the possible difference in test data. But the svm trained on word2vec embeddings clearly performs better than tf-idf based one as the word2vec model can incorporate semantic relationship between words into its embeddings unlike the tf-idf model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd2e02b",
   "metadata": {},
   "source": [
    "### 4a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e592f6ae",
   "metadata": {},
   "source": [
    "A function to create a perceptron model and train it based on the input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8563c8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multilayer perceptron\n",
    "\n",
    "## The multilayer perceptron class\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self,inputsize,outputsize,hidden_layer_1,hidden_layer_2):\n",
    "        super(NeuralNet,self).__init__()\n",
    "        self.linear1=nn.Linear(inputsize,hidden_layer_1)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.linear2=nn.Linear(hidden_layer_1,hidden_layer_2)\n",
    "        self.linear3=nn.Linear(hidden_layer_2,outputsize)\n",
    "\n",
    "    def forward(self,x):\n",
    "        l1=self.linear1(x)\n",
    "        a1=self.relu(l1)\n",
    "        l2=self.linear2(a1)\n",
    "        a2=self.relu(l2)\n",
    "        l3=self.linear3(a2)\n",
    "        return l3\n",
    "\n",
    "def mlp(epochs,batch_size,h1,h2,classes,lr,ipsize,Xtrain_embeddings,Xtest_embeddings,ytrain_en):\n",
    "    ## Hyperparameters which are taken as an input to the function\n",
    "    epochs=epochs\n",
    "    batch_size=batch_size\n",
    "    hidden_layer_1=h1\n",
    "    hidden_layer_2=h2\n",
    "    num_classes=classes\n",
    "    learning_rate=lr\n",
    "    inputsize=ipsize\n",
    "    Xtrain_embeddings_tensor=torch.tensor(Xtrain_embeddings)\n",
    "    Xtest_embeddings_tensor=torch.tensor(Xtest_embeddings)\n",
    "    ytrain_tensor=torch.tensor(ytrain_en)\n",
    "\n",
    "    model=NeuralNet(inputsize,num_classes,h1,h2)\n",
    "    loss=nn.CrossEntropyLoss()\n",
    "    optimizer= torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    ## Training the model\n",
    "    batch_size=100\n",
    "    i=0\n",
    "    l=None\n",
    "    for epoch in range(epochs):\n",
    "        i=0\n",
    "        while i<80000:\n",
    "            outputs=model(Xtrain_embeddings_tensor[i:min(i+100,len(Xtrain_embeddings_tensor))])\n",
    "            l=loss(outputs,ytrain_tensor[i:min(i+100,len(Xtrain_embeddings_tensor))])\n",
    "            i+=100\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        if epoch%50==0:\n",
    "            \n",
    "            print(\"epoch \",epoch,\" loss \",l.item())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b4ea339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savemlp(model,name):\n",
    "    torch.save(model,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9c9c26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(Xtest,ytest,model,sublist=True):\n",
    "    Xtest_embeddings=torch.tensor(Xtest)\n",
    "    corr=0\n",
    "    with torch.no_grad(): \n",
    "        for sample in range(len(Xtest_embeddings)):\n",
    "            outputs=model(Xtest_embeddings[sample])\n",
    "            outputs=outputs.tolist()\n",
    "            if sublist:\n",
    "                pred=outputs[0].index(max(outputs[0]))+1\n",
    "            else:\n",
    "                pred=outputs.index(max(outputs))+1\n",
    "            if pred==ytest[sample]:\n",
    "                corr+=1\n",
    "    acc=corr/len(Xtest_embeddings)\n",
    "    print(\"Accuracy:\",acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6122b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the processed data, split it into a train_test split of 80%/20% and get their corresponding word2vec word embeddings\n",
    "\n",
    "processed_data=preprocess_data(new_df)\n",
    "Xtrain,Xtest,ytrain,ytest=train_test_split(processed_data['input'],processed_data['target'],shuffle=True,test_size=0.2)\n",
    "Xtrain_embeddings,Xtest_embeddings,ytrain_en,_=get_word2vec_embeddings(Xtrain,Xtest,ytrain,ytest,w2model_pretrained=w2model_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc0d80a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Pytorch_practice\\lib\\site-packages\\ipykernel_launcher.py:29: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0  loss  1.5616744756698608\n",
      "epoch  50  loss  1.1872563362121582\n",
      "epoch  100  loss  1.1553277969360352\n",
      "epoch  150  loss  1.1375845670700073\n",
      "epoch  200  loss  1.13045072555542\n",
      "epoch  250  loss  1.1223459243774414\n"
     ]
    }
   ],
   "source": [
    "mlp_model1=mlp(300,100,50,10,5,0.0001,300,Xtrain_embeddings,Xtest_embeddings,ytrain_en)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5f6f284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.49235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49235"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(Xtest_embeddings,ytest,mlp_model1,sublist=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815ad1bc",
   "metadata": {},
   "source": [
    "### 4b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d86b1ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MLP with 10 words in each sample whose word vectors are concatenated with each other\n",
    "processed_data=preprocess_data(new_df,inword2vec=True,wordvec=w2model_pretrained)\n",
    "Xtrain,Xtest,ytrain,ytest=train_test_split(processed_data['input'],processed_data['target'],test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d2bbe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the corresponding word embeddings where in each sentence only 10 words are selected and they are concatenated with each other\n",
    "\n",
    "Xtrain_embeddings,Xtest_embeddings,ytrain_en,ytest=get_word2vec_embeddings(Xtrain,Xtest,ytrain,ytest,skipten=True,fixedsize=10,long=False,concat=True,w2model_pretrained=w2model_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7064d88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0  loss  1.4550485610961914\n",
      "epoch  50  loss  1.038692593574524\n",
      "epoch  100  loss  0.8234057426452637\n",
      "epoch  150  loss  0.6263967752456665\n",
      "epoch  200  loss  0.42741626501083374\n",
      "epoch  250  loss  0.28945839405059814\n"
     ]
    }
   ],
   "source": [
    "mlp_model2=mlp(epochs=300,batch_size=100,h1=50,h2=10,classes=5,lr=0.0001,ipsize=3000,Xtrain_embeddings=Xtrain_embeddings,Xtest_embeddings=Xtest_embeddings,ytrain_en=ytrain_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f0ee8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.33635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.33635"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(Xtest_embeddings,ytest,mlp_model2,sublist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260f39ce",
   "metadata": {},
   "source": [
    "Its clear from the above results that, the model in which word vectors in a sample are concatenated with each other performs \n",
    "poorly than the model where each wordvector is present in a separate list in a train sample list. This could be because, concatenating the word vectors could result in potential information loss, whereas not concatenating them could enable the perceptron model to learn possible relations between the words and therefore perform better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fc9abc",
   "metadata": {},
   "source": [
    "### 5a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a24c2c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the processed data and perform a train test split with a train to test ratio of 80% and 20%. The corresponding word2vec\n",
    "## embeddings are also fetched where the number of words in a sentence is limited to 20 and the vectors are not concatenated with\n",
    "## each other. If 20 words are not present in a train sample, then the remaining word vectors for the sample are 0 vectors of \n",
    "## size 300 as that is the embedding size for all the rest of the words.\n",
    "\n",
    "processed_data=preprocess_data(new_df,inword2vec=True,wordvec=w2model_pretrained,fixedsize=13)\n",
    "Xtrain,Xtest,ytrain,ytest=train_test_split(processed_data['input'],processed_data['target'],test_size=0.2,shuffle=True)\n",
    "Xtrain_embeddings,Xtest_embeddings,ytrain_en,ytest=get_word2vec_embeddings(Xtrain,Xtest,ytrain,ytest,fixedsize=20,skipten=True,w2model_pretrained=w2model_pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "942ee725",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert the embeddings list to a torch tensor\n",
    "\n",
    "Xtrain_tensor=torch.tensor(Xtrain_embeddings)\n",
    "Xtest_tensor=torch.tensor(Xtest_embeddings)\n",
    "ytrain_tensor=torch.tensor(ytrain_en)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbab554",
   "metadata": {},
   "source": [
    "The following function creates and trains a simple RNN over the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79811f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  1.613768458366394\n",
      "epoch:  10  loss:  1.2795147895812988\n",
      "epoch:  20  loss:  1.274950385093689\n",
      "epoch:  30  loss:  1.2476483583450317\n",
      "epoch:  40  loss:  1.2182153463363647\n",
      "epoch:  50  loss:  1.1994259357452393\n",
      "epoch:  60  loss:  1.1851786375045776\n",
      "epoch:  70  loss:  1.1754580736160278\n",
      "epoch:  80  loss:  1.1693321466445923\n",
      "epoch:  90  loss:  1.1653435230255127\n"
     ]
    }
   ],
   "source": [
    "### RNN\n",
    "\n",
    "embedding_len=300\n",
    "hidden_layer=20\n",
    "classes=5\n",
    "lr=0.0001\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self,embedding_len,hidden_layer,classes,lr,nlayers):\n",
    "        super(RNN,self).__init__()\n",
    "        self.embedding_len=embedding_len\n",
    "        self.hidden_layer=hidden_layer\n",
    "        self.nlayers=nlayers\n",
    "        self.rnn=nn.RNN(self.embedding_len,self.hidden_layer,self.nlayers,batch_first=True)\n",
    "        self.linear=nn.Linear(self.hidden_layer,classes)\n",
    "    \n",
    "    def forward(self,inp):\n",
    "        hidden=self.init_hidden(inp.shape[0])\n",
    "        outputs,hidden=self.rnn(inp,hidden)\n",
    "        outputs=outputs[:,-1,:]\n",
    "        outputs=self.linear(outputs)\n",
    "        return outputs\n",
    "        \n",
    "    \n",
    "    def init_hidden(self,batchsize):\n",
    "        return torch.zeros(self.nlayers,batchsize,self.hidden_layer)\n",
    "\n",
    "model=RNN(embedding_len,hidden_layer,classes,lr,1)\n",
    "crit=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=lr)\n",
    "epochs=100\n",
    "batch_size=100\n",
    "loss=None\n",
    "for epoch in range(epochs):\n",
    "    i=0\n",
    "    while i<len(Xtrain_tensor):\n",
    "        outputs=model(Xtrain_tensor[i:i+batch_size])\n",
    "        loss=crit(outputs,ytrain_tensor[i:i+batch_size])\n",
    "        i+=batch_size\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    if epoch%10==0:\n",
    "        print(\"epoch: \",epoch,\" loss: \",loss.item())\n",
    "\n",
    "\n",
    "rnn_model=model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f41d3a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Pytorch_practice\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.47365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.47365"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_seq_accuracy(Xtest,ytest,model,sublist=True):\n",
    "    Xtest_embeddings=torch.tensor(Xtest)\n",
    "    corr=0\n",
    "    with torch.no_grad(): \n",
    "        for sample in range(len(Xtest_embeddings)):\n",
    "            outputs=model(torch.unsqueeze(torch.tensor(Xtest_embeddings[sample]),dim=0))\n",
    "            outputs=outputs.tolist()\n",
    "            if sublist:\n",
    "                pred=outputs[0].index(max(outputs[0]))+1\n",
    "            else:\n",
    "                pred=outputs.index(max(outputs))+1\n",
    "            if pred==ytest[sample]:\n",
    "                corr+=1\n",
    "    acc=corr/len(Xtest_embeddings)\n",
    "    print(\"Accuracy:\",acc)\n",
    "    return acc\n",
    "get_seq_accuracy(Xtest_embeddings,ytest,rnn_model,sublist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9842b329",
   "metadata": {},
   "source": [
    "### 5b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf023df0",
   "metadata": {},
   "source": [
    "The following function creates and trains a GRU over the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51b225eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  1.5861648321151733\n",
      "epoch:  10  loss:  1.2261070013046265\n",
      "epoch:  20  loss:  1.1830229759216309\n",
      "epoch:  30  loss:  1.1608710289001465\n",
      "epoch:  40  loss:  1.147367000579834\n",
      "epoch:  50  loss:  1.1383633613586426\n",
      "epoch:  60  loss:  1.1313523054122925\n",
      "epoch:  70  loss:  1.1249290704727173\n",
      "epoch:  80  loss:  1.1184208393096924\n",
      "epoch:  90  loss:  1.1115845441818237\n"
     ]
    }
   ],
   "source": [
    "### GRU\n",
    "embedding_len=300\n",
    "hidden_layer=20\n",
    "classes=5\n",
    "lr=0.0001\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self,embedding_len,hidden_layer,classes,lr,nlayers):\n",
    "        super(GRU,self).__init__()\n",
    "        self.embedding_len=embedding_len\n",
    "        self.hidden_layer=hidden_layer\n",
    "        self.nlayers=nlayers\n",
    "        self.gru=nn.GRU(self.embedding_len,self.hidden_layer,self.nlayers,batch_first=True)\n",
    "        self.linear=nn.Linear(self.hidden_layer,classes)\n",
    "    \n",
    "    def forward(self,inp):\n",
    "        hidden=self.init_hidden(inp.shape[0])\n",
    "        outputs,hidden=self.gru(inp,hidden)\n",
    "        outputs=outputs[:,-1,:]\n",
    "        outputs=self.linear(outputs)\n",
    "        return outputs\n",
    "        \n",
    "    \n",
    "    def init_hidden(self,batchsize):\n",
    "        return torch.zeros(self.nlayers,batchsize,self.hidden_layer)\n",
    "\n",
    "model=GRU(embedding_len,hidden_layer,classes,lr,1)\n",
    "crit=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=lr)\n",
    "epochs=100\n",
    "batch_size=100\n",
    "loss=None\n",
    "for epoch in range(epochs):\n",
    "    i=0\n",
    "    while i<len(Xtrain_tensor):\n",
    "        outputs=model(Xtrain_tensor[i:i+batch_size])\n",
    "        loss=crit(outputs,ytrain_tensor[i:i+batch_size])\n",
    "        i+=batch_size\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    if epoch%10==0:\n",
    "        print(\"epoch: \",epoch,\" loss: \",loss.item())\n",
    "\n",
    "\n",
    "gru_model=model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a1a9b204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Pytorch_practice\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.50405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.50405"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_seq_accuracy(Xtest_embeddings,ytest,gru_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4232aa",
   "metadata": {},
   "source": [
    "Its clear from the accuracy results obtained in the sections 5a and 5b that the GRU outperforms the RNN over the given dataset. This could especially be because GRU handles the disadvantages that RNN has such as vanishing gradient problem with its two gate based working. This could enable the GRU to learn long term dependencies between the words which the RNN cannot and thus helps the GRU yield better results than the RNN"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8c4e390ee733427358890da59b39fc450134967e07c5b4a6c5f8864a0a52534d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
